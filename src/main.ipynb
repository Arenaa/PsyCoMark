{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c71e8213",
   "metadata": {},
   "source": [
    "# PsyCoMark Multi-task Training: Marker Types + Conspiracy Detection\n",
    "\n",
    "Joint training with DistilRoBERTa for:\n",
    "- Token classification (BIO tags for marker types)\n",
    "- Document classification (conspiracy: Yes/No/Can't tell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8a7c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup (run first)\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TORCHVISION\"] = \"1\"   # avoid torchvision import path\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"    # quieter logs\n",
    "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607748a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip -q install transformers==4.46.1 datasets==2.20.0 accelerate==0.34.2 seqeval==1.2.2 scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435bf1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json, random, os, math\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig, AutoModel, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    ")\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import f1_score as sk_f1_score, classification_report as sk_classification_report\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "import transformers, datasets\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"datasets:\", datasets.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e6bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SEED = 42\n",
    "MODEL_NAME = \"distilroberta-base\"\n",
    "MAX_LEN = 512\n",
    "DOC_LABELS = [\"No\", \"Yes\", \"Can't tell\"]  # order matters\n",
    "MARKER_TYPES = [\"Actor\", \"Action\", \"Victim\", \"Evidence\", \"Effect\"]\n",
    "BIO_LABELS = [\"O\"] + [f\"{p}-{t}\" for t in MARKER_TYPES for p in [\"B\",\"I\"]]\n",
    "\n",
    "# Loss weights - boost token learning\n",
    "LAMBDA_TOKEN = 3.0  # Increased from 1.0\n",
    "LAMBDA_DOC = 1.0\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 4\n",
    "LR = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "print(f\"BIO labels ({len(BIO_LABELS)}): {BIO_LABELS}\")\n",
    "print(f\"Doc labels ({len(DOC_LABELS)}): {DOC_LABELS}\")\n",
    "print(f\"Loss weights: token={LAMBDA_TOKEN}, doc={LAMBDA_DOC}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6766e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split data\n",
    "DATA_PATH = \"/content/train_rehydrated.jsonl\"\n",
    "assert os.path.exists(DATA_PATH), \"Upload train_rehydrated.jsonl to /content first.\"\n",
    "\n",
    "rows = [json.loads(l) for l in open(DATA_PATH, \"r\", encoding=\"utf-8\")]\n",
    "print(f\"Total rows: {len(rows)}\")\n",
    "\n",
    "# Stratified 70/15/15 split by 'conspiracy'\n",
    "by_label = defaultdict(list)\n",
    "for r in rows:\n",
    "    by_label[r[\"conspiracy\"]].append(r)\n",
    "\n",
    "splits = {\"train\": [], \"val\": [], \"test\": []}\n",
    "for lbl, items in by_label.items():\n",
    "    random.shuffle(items)\n",
    "    n = len(items)\n",
    "    n_train = int(0.70 * n)\n",
    "    n_val = int(0.15 * n)\n",
    "    train, val, test = items[:n_train], items[n_train:n_train+n_val], items[n_train+n_val:]\n",
    "    splits[\"train\"].extend(train)\n",
    "    splits[\"val\"].extend(val)\n",
    "    splits[\"test\"].extend(test)\n",
    "\n",
    "def summarize_split(name, data):\n",
    "    c = Counter(r[\"conspiracy\"] for r in data)\n",
    "    print(f\"{name}: n={len(data)}  {dict(c)}\")\n",
    "\n",
    "summarize_split(\"train\", splits[\"train\"])\n",
    "summarize_split(\"val\", splits[\"val\"])\n",
    "summarize_split(\"test\", splits[\"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3075ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mappings\n",
    "doc_label2id = {c:i for i,c in enumerate(DOC_LABELS)}\n",
    "doc_id2label = {i:c for c,i in doc_label2id.items()}\n",
    "\n",
    "tag2id = {t:i for i,t in enumerate(BIO_LABELS)}\n",
    "id2tag = {i:t for t,i in tag2id.items()}\n",
    "\n",
    "print(\"Doc label mapping:\", doc_label2id)\n",
    "print(\"BIO tag mapping (first 10):\", dict(list(tag2id.items())[:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae6dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, token=False)\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Special tokens: {tokenizer.special_tokens_map}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672cdc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Char span to BIO alignment\n",
    "def char_spans_to_bio(text: str, markers: List[Dict[str, Any]], encoding) -> List[int]:\n",
    "    # Base O tags\n",
    "    labels = [\"O\"] * len(encoding.input_ids)\n",
    "\n",
    "    # Token char spans\n",
    "    token2span = encoding.encodings[0].offsets  # list of (start,end) per token\n",
    "    \n",
    "    # Build per-token tag\n",
    "    for m in markers or []:\n",
    "        ttype = m.get(\"type\")\n",
    "        if ttype not in MARKER_TYPES: \n",
    "            continue\n",
    "        s_char = m.get(\"startIndex\")\n",
    "        e_char = m.get(\"endIndex\")\n",
    "        if not isinstance(s_char, int) or not isinstance(e_char, int):\n",
    "            continue\n",
    "            \n",
    "        # Mark tokens overlapping this char span\n",
    "        inside = False\n",
    "        for ti, (cs, ce) in enumerate(token2span):\n",
    "            if cs is None or ce is None or cs == ce:\n",
    "                continue\n",
    "            # overlap if [cs,ce) intersects [s_char,e_char)\n",
    "            if max(cs, s_char) < min(ce, e_char):\n",
    "                prefix = \"B\" if not inside else \"I\"\n",
    "                cand = f\"{prefix}-{ttype}\"\n",
    "                # If already labeled this token for a different type, keep first (simple heuristic)\n",
    "                if labels[ti] == \"O\":\n",
    "                    labels[ti] = cand\n",
    "                inside = True\n",
    "            elif inside:\n",
    "                # span ended\n",
    "                break\n",
    "\n",
    "    # Map to ids; ensure special tokens get O\n",
    "    label_ids = []\n",
    "    for ti, lid in enumerate(labels):\n",
    "        if token2span[ti] == (0, 0) or token2span[ti][0] is None:\n",
    "            label_ids.append(tag2id[\"O\"])\n",
    "        else:\n",
    "            label_ids.append(tag2id.get(lid, tag2id[\"O\"]))\n",
    "    return label_ids\n",
    "\n",
    "def prepare_examples(example):\n",
    "    text = example[\"text\"]\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    # Token labels\n",
    "    token_label_ids = char_spans_to_bio(text, example.get(\"markers\", []), enc)\n",
    "    # Doc label\n",
    "    doc_label_id = doc_label2id[example[\"conspiracy\"]]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": enc[\"input_ids\"],\n",
    "        \"attention_mask\": enc[\"attention_mask\"],\n",
    "        \"token_labels\": token_label_ids,\n",
    "        \"doc_label\": doc_label_id,\n",
    "    }\n",
    "\n",
    "def to_hf_dataset(items):\n",
    "    ds = Dataset.from_list(items)\n",
    "    return ds.map(prepare_examples, remove_columns=ds.column_names)\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": to_hf_dataset(splits[\"train\"]),\n",
    "    \"validation\": to_hf_dataset(splits[\"val\"]),\n",
    "    \"test\": to_hf_dataset(splits[\"test\"]),\n",
    "})\n",
    "\n",
    "print(\"Dataset created successfully\")\n",
    "print(f\"Train: {len(ds['train'])} examples\")\n",
    "print(f\"Validation: {len(ds['validation'])} examples\")\n",
    "print(f\"Test: {len(ds['test'])} examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553038e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-task collator\n",
    "class MultiTaskCollator:\n",
    "    def __init__(self, tokenizer, label_pad_id=-100):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_pad_id = label_pad_id\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # Extract all fields\n",
    "        token_labels = [f[\"token_labels\"] for f in features]\n",
    "        doc_labels = [f[\"doc_label\"] for f in features]\n",
    "        input_ids_list = [f[\"input_ids\"] for f in features]\n",
    "        attention_mask_list = [f[\"attention_mask\"] for f in features]\n",
    "        \n",
    "        # Pad input_ids and attention_mask\n",
    "        max_len = max(len(ids) for ids in input_ids_list)\n",
    "        \n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "        for ids, mask in zip(input_ids_list, attention_mask_list):\n",
    "            pad_len = max_len - len(ids)\n",
    "            input_ids.append(ids + [self.tokenizer.pad_token_id] * pad_len)\n",
    "            attention_mask.append(mask + [0] * pad_len)\n",
    "        \n",
    "        # Pad token labels\n",
    "        max_label_len = max(len(tl) for tl in token_labels)\n",
    "        padded_labels = []\n",
    "        for tl in token_labels:\n",
    "            pad_len = max_label_len - len(tl)\n",
    "            padded_labels.append(tl + [self.label_pad_id] * pad_len)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(padded_labels, dtype=torch.long),\n",
    "            \"doc_labels\": torch.tensor(doc_labels, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "collator = MultiTaskCollator(tokenizer)\n",
    "print(\"Collator created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513dd174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-task model with improved token learning\n",
    "class MultiTaskDistilRoberta(torch.nn.Module):\n",
    "    def __init__(self, base_model_name, num_token_labels, num_doc_labels):\n",
    "        super().__init__()\n",
    "        self.config = AutoConfig.from_pretrained(base_model_name, token=False)\n",
    "        self.encoder = AutoModel.from_pretrained(base_model_name, config=self.config, token=False)\n",
    "        hidden = self.config.hidden_size\n",
    "        \n",
    "        # Heads\n",
    "        self.token_classifier = torch.nn.Linear(hidden, num_token_labels)\n",
    "        self.doc_classifier = torch.nn.Linear(hidden, num_doc_labels)\n",
    "        \n",
    "        # Losses with class weights for better learning\n",
    "        self.loss_token = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        \n",
    "        # Doc class weights (inverse frequency)\n",
    "        doc_weights = torch.tensor([0.72, 0.93, 1.83], dtype=torch.float)\n",
    "        self.loss_doc = torch.nn.CrossEntropyLoss(weight=doc_weights)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, doc_labels=None):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden = outputs.last_hidden_state  # (B,T,H)\n",
    "\n",
    "        # Token head\n",
    "        token_logits = self.token_classifier(last_hidden)  # (B,T,Ct)\n",
    "\n",
    "        # Doc head: mean-pool masked\n",
    "        mask = attention_mask.unsqueeze(-1)  # (B,T,1)\n",
    "        summed = (last_hidden * mask).sum(dim=1)  # (B,H)\n",
    "        lengths = mask.sum(dim=1).clamp(min=1)   # (B,1)\n",
    "        pooled = summed / lengths\n",
    "        doc_logits = self.doc_classifier(pooled)  # (B,Cd)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None and doc_labels is not None:\n",
    "            lt = self.loss_token(token_logits.view(-1, token_logits.size(-1)), labels.view(-1))\n",
    "            ld = self.loss_doc(doc_logits, doc_labels)\n",
    "            loss = LAMBDA_TOKEN * lt + LAMBDA_DOC * ld\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": token_logits, \"doc_logits\": doc_logits}\n",
    "\n",
    "model = MultiTaskDistilRoberta(MODEL_NAME, num_token_labels=len(BIO_LABELS), num_doc_labels=len(DOC_LABELS))\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"Token head: {model.token_classifier}\")\n",
    "print(f\"Doc head: {model.doc_classifier}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89cb477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics: Macro Overlap F1 (IoU>=0.5) for markers, Weighted F1 for doc\n",
    "import itertools\n",
    "from sklearn.metrics import f1_score as skf1\n",
    "\n",
    "def chunks_to_spans(tags):\n",
    "    # Convert BIO tags to spans: (start_idx, end_idx_inclusive, type)\n",
    "    spans = []\n",
    "    start, ttype = None, None\n",
    "    for i, tag in enumerate(tags + [\"O\"]):  # sentinel O\n",
    "        if tag.startswith(\"B-\"):\n",
    "            if start is not None:\n",
    "                spans.append((start, i-1, ttype))\n",
    "            start, ttype = i, tag[2:]\n",
    "        elif tag.startswith(\"I-\"):\n",
    "            if start is None:\n",
    "                # treat as B-\n",
    "                start, ttype = i, tag[2:]\n",
    "        else:\n",
    "            if start is not None:\n",
    "                spans.append((start, i-1, ttype))\n",
    "                start, ttype = None, None\n",
    "    return spans\n",
    "\n",
    "def iou(a_start, a_end, b_start, b_end):\n",
    "    inter = max(0, min(a_end, b_end) - max(a_start, b_start) + 1)\n",
    "    union = (a_end - a_start + 1) + (b_end - b_start + 1) - inter\n",
    "    return inter / union if union > 0 else 0.0\n",
    "\n",
    "def macro_overlap_f1(true_tags_list, pred_tags_list, types=MARKER_TYPES, iou_thresh=0.5):\n",
    "    # Build per-type precision/recall via optimal matching (greedy by IoU)\n",
    "    per_type_f1 = {}\n",
    "    for t in types:\n",
    "        tp = fp = fn = 0\n",
    "        for true_tags, pred_tags in zip(true_tags_list, pred_tags_list):\n",
    "            true_spans = [(s,e,tt) for (s,e,tt) in chunks_to_spans(true_tags) if tt == t]\n",
    "            pred_spans = [(s,e,tt) for (s,e,tt) in chunks_to_spans(pred_tags) if tt == t]\n",
    "            used = set()\n",
    "            # match preds to trues greedily by IoU\n",
    "            for ps,pe,_ in pred_spans:\n",
    "                best_iou, best_j = 0.0, -1\n",
    "                for j,(ts,te,_) in enumerate(true_spans):\n",
    "                    if j in used: continue\n",
    "                    score = iou(ps,pe,ts,te)\n",
    "                    if score > best_iou:\n",
    "                        best_iou, best_j = score, j\n",
    "                if best_iou >= iou_thresh:\n",
    "                    tp += 1; used.add(best_j)\n",
    "                else:\n",
    "                    fp += 1\n",
    "            fn += (len(true_spans) - len(used))\n",
    "        prec = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
    "        rec  = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
    "        f1 = 2*prec*rec/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "        per_type_f1[t] = f1\n",
    "    macro_f1 = float(np.mean(list(per_type_f1.values()))) if per_type_f1 else 0.0\n",
    "    return macro_f1, per_type_f1\n",
    "\n",
    "def compute_metrics_fn(eval_pred):\n",
    "    # Unpack EvalPrediction\n",
    "    if hasattr(eval_pred, \"predictions\"):\n",
    "        logits = eval_pred.predictions\n",
    "        labels = eval_pred.label_ids\n",
    "    else:\n",
    "        logits, labels = eval_pred\n",
    "    if isinstance(logits, (tuple, list)):\n",
    "        logits = logits[0]\n",
    "    if isinstance(labels, (tuple, list)):\n",
    "        labels = labels[0]\n",
    "    logits = np.asarray(logits)\n",
    "    labels = np.asarray(labels)\n",
    "    if logits.ndim < 2:\n",
    "        return {\"token_macro_overlap_f1\": 0.0}\n",
    "    preds = logits.argmax(axis=-1) if logits.ndim >= 3 else logits\n",
    "\n",
    "    # Build tag sequences\n",
    "    pred_tags, true_tags = [], []\n",
    "    for p_seq, l_seq in zip(preds, labels):\n",
    "        seq_pred, seq_true = [], []\n",
    "        p_seq = np.ravel(p_seq)\n",
    "        l_seq = np.ravel(l_seq)\n",
    "        for p_id, l_id in zip(p_seq, l_seq):\n",
    "            l_id = int(l_id)\n",
    "            if l_id == -100:\n",
    "                continue\n",
    "            seq_pred.append(id2tag[int(p_id)])\n",
    "            seq_true.append(id2tag[l_id])\n",
    "        if seq_true:\n",
    "            pred_tags.append(seq_pred)\n",
    "            true_tags.append(seq_true)\n",
    "\n",
    "    macro_f1, per_type = macro_overlap_f1(true_tags, pred_tags)\n",
    "    # Return only the primary token metric for trainer\n",
    "    out = {\"token_macro_overlap_f1\": macro_f1}\n",
    "    # Optionally include per-type\n",
    "    for k,v in per_type.items():\n",
    "        out[f\"f1_{k}\"] = v\n",
    "    return out\n",
    "\n",
    "print(\"Metrics updated: Macro Overlap F1 (IoU>=0.5) for markers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb2f88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/content/outputs\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_token_macro_overlap_f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    "    dataloader_drop_last=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "class WrappedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        doc_labels = inputs.get(\"doc_labels\")\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            labels=labels,\n",
    "            doc_labels=doc_labels,\n",
    "        )\n",
    "        loss = outputs[\"loss\"]\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = WrappedTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics_fn,\n",
    ")\n",
    "\n",
    "print(\"Trainer created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a1108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3959ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "# Competition metric for doc task: Weighted F1 on binary mapping (Can't tell -> No)\n",
    "from sklearn.metrics import f1_score as sk_f1, classification_report as sk_clf_report\n",
    "\n",
    "def _map_to_binary(labels: list[int]) -> list[int]:\n",
    "    # DOC_LABELS = [\"No\", \"Yes\", \"Can't tell\"]\n",
    "    # Map: No->0, Yes->1, Can't tell->0\n",
    "    return [0 if DOC_LABELS[l] in (\"No\", \"Can't tell\") else 1 for l in labels]\n",
    "\n",
    "def evaluate_doc(dataset):\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collator)\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            doc_labels = batch[\"doc_labels\"].to(device)\n",
    "            out = model(input_ids=input_ids, attention_mask=attention_mask, labels=None, doc_labels=None)\n",
    "            pred = out[\"doc_logits\"].argmax(dim=-1)\n",
    "            preds.extend(pred.cpu().tolist())\n",
    "            trues.extend(doc_labels.cpu().tolist())\n",
    "\n",
    "    # Map to binary\n",
    "    preds_bin = _map_to_binary(preds)\n",
    "    trues_bin = _map_to_binary(trues)\n",
    "\n",
    "    weighted_f1 = sk_f1(trues_bin, preds_bin, average=\"weighted\")\n",
    "    print(f\"Doc Weighted-F1 (binary Yes/No, Can't tell->No): {weighted_f1:.4f}\")\n",
    "    print(sk_clf_report(trues_bin, preds_bin, target_names=[\"No\",\"Yes\"]))\n",
    "    return weighted_f1\n",
    "\n",
    "print(\"Evaluation functions updated (Weighted F1, binary mapping)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e45360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation results\n",
    "print(\"=== VALIDATION RESULTS ===\")\n",
    "val_token_metrics = trainer.evaluate()\n",
    "print(\"Validation (token) metrics:\", val_token_metrics)\n",
    "\n",
    "print(\"\\nValidation (doc) metrics:\")\n",
    "val_doc_f1 = evaluate_doc(ds[\"validation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e995372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test results\n",
    "print(\"=== TEST RESULTS ===\")\n",
    "test_token_metrics = trainer.evaluate(ds[\"test\"])\n",
    "print(\"Test (token) metrics:\", test_token_metrics)\n",
    "\n",
    "print(\"\\nTest (doc) metrics:\")\n",
    "test_doc_f1 = evaluate_doc(ds[\"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=== FINAL SUMMARY ===\")\n",
    "print(f\"Token F1 - Val: {val_token_metrics['eval_token_f1']:.4f}, Test: {test_token_metrics['eval_token_f1']:.4f}\")\n",
    "print(f\"Doc Macro-F1 - Val: {val_doc_f1:.4f}, Test: {test_doc_f1:.4f}\")\n",
    "print(f\"Loss weights used: token={LAMBDA_TOKEN}, doc={LAMBDA_DOC}\")\n",
    "print(f\"Training completed with {EPOCHS} epochs\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
